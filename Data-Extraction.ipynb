{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction | CS324\n",
      "Link\n",
      "Search\n",
      "Menu\n",
      "Expand\n",
      "Document\n",
      "CS324\n",
      "HomeCalendarLecturesIntroductionCapabilitiesHarms IHarms IIDataSecurityLegalityModelingTrainingParallelismScaling lawsSelective architecturesAdaptationEnvironmental impactPaper reviewsPaper discussionsProjects This site uses Just the Docs, a documentation theme for Jekyll.\n",
      "LecturesIntroduction \\[\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}}\\]Welcome to CS324! This is a new course on understanding and developing large language models.What is a language model?A brief historyWhy does this course exist?Structure of this course\n",
      "What is a language model?The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):\\[p(x_1, \\dots, x_L).\\]The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):\\[p(\\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}) = 0.02,\\] \\[p(\\nl{the}, \\nl{cheese}, \\nl{ate}, \\nl{the}, \\nl{mouse}) = 0.01,\\] \\[p(\\nl{mouse}, \\nl{the}, \\nl{the}, \\nl{cheese}, \\nl{ate}) = 0.0001.\\]Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:\\[x_{1:L} \\sim p.\\]How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.\n",
      "Autoregressive language modelsA common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:\\[p(x_{1:L}) = p(x_1) p(x_2 \\mid x_1) p(x_3 \\mid x_1, x_2) \\cdots p(x_L \\mid x_{1:L-1}) = \\prod_{i=1}^L p(x_i \\mid x_{1:i-1}).\\]For example (demo):\\[\\begin{align*} p(\\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}) = \\, & p(\\nl{the}) \\\\ & p(\\nl{mouse} \\mid \\nl{the}) \\\\ & p(\\nl{ate} \\mid \\nl{the}, \\nl{mouse}) \\\\ & p(\\nl{the} \\mid \\nl{the}, \\nl{mouse}, \\nl{ate}) \\\\ & p(\\nl{cheese} \\mid \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}). \\end{align*}\\]In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:\\[\\text{for } i = 1, \\dots, L: \\\\ \\hspace{1in} x_i \\sim p(x_i \\mid x_{1:i-1})^{1/T},\\]where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:\\(T = 0\\): deterministically choose the most probable token \\(x_i\\) at each position \\(i\\)\\(T = 1\\): sample “normally” from the pure language model\\(T = \\infty\\): sample from a uniform distribution over the entire vocabulary \\(\\sV\\)However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:\\[p(\\nl{cheese}) = 0.4, \\quad\\quad\\quad p(\\nl{mouse}) = 0.6\\] \\[p_{T=0.5}(\\nl{cheese}) = 0.31, \\quad\\quad\\quad p_{T=0.5}(\\nl{mouse}) = 0.69\\] \\[p_{T=0.2}(\\nl{cheese}) = 0.12, \\quad\\quad\\quad p_{T=0.2}(\\nl{mouse}) = 0.88\\] \\[p_{T=0}(\\nl{cheese}) = 0, \\quad\\quad\\quad p_{T=0}(\\nl{mouse}) = 1\\]Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):\\[\\underbrace{\\nl{the}, \\nl{mouse}, \\nl{ate}}_\\text{prompt} \\generate{T=0} \\underbrace{\\nl{the}, \\nl{cheese}}_\\text{completion}.\\]If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\).As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.\n",
      "SummaryA language model is a probability distribution \\(p\\) over sequences \\(x_{1:L}\\).Intuitively, a good language model should have linguistic capabilities and world knowledge.An autoregressive language model allows for efficient generation of a completion \\(x_{i+1:L}\\) given a prompt \\(x_{1:i}\\).The temperature can be used to control the amount of variability in generation.\n",
      "A brief history\n",
      "Information theory, entropy of English, n-gram modelsInformation theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as\\[H(p) = \\sum_x p(x) \\log \\frac{1}{p(x)}.\\]The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:\\[\\nl{the mouse ate the cheese} \\Rightarrow 0001110101.\\]The lower the entropy, the more “structured” the sequence is, and the shorter the code length.Intuitively, \\(\\log \\frac{1}{p(x)}\\) is the length of the code used to represent an element \\(x\\) that occurs with probability \\(p(x)\\).If \\(p(x) = \\frac{1}{8}\\), we should allocate \\(\\log_2(8) = 3\\) bits (equivalently, \\(\\log(8) = 2.08\\) nats).Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\(p\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).Shannon also defined cross entropy:\\[H(p, q) = \\sum_x p(x) \\log \\frac{1}{q(x)},\\]which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),\\[H(p, q) \\ge H(p),\\]which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English.So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:\\[\\nl{the mouse ate my ho_}\\]Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.\n",
      "N-gram models for downstream applicationsLanguage models became first used in practical applications that required generation of text:speech recognition in the 1970s (input: acoustic signal, output: text), andmachine translation in the 1990s (input: text in a source language, output: text in a target language).Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:We posit that there is some text sampled from some distribution \\(p\\).This text becomes realized to speech (acoustic signals).Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule:\\[p(\\text{text} \\mid \\text{speech}) \\propto \\underbrace{p(\\text{text})}_\\text{language model} \\underbrace{p(\\text{speech} \\mid \\text{text})}_\\text{acoustic model}.\\]Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:\\[p(x_i \\mid x_{1:i-1}) = p(x_i \\mid x_{i-(n-1):i-1}).\\]For example, a trigram (\\(n=3\\)) model would define:\\[p(\\nl{cheese} \\mid \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}) = p(\\nl{cheese} \\mid \\nl{ate}, \\nl{the}).\\]These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:\\[\\nl{Stanford has a new course on large language models. It will be taught by ___}\\]If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora):\\[\\text{count}(\\nl{Stanford}, \\nl{has}, \\nl{a}, \\nl{new}, \\nl{course}, \\nl{on}, \\nl{large}, \\nl{language}, \\nl{models}) = 0.\\]As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem.\n",
      "Neural language modelsAn important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:\\[p(\\nl{cheese} \\mid \\nl{ate}, \\nl{the}) = \\text{some-neural-network}(\\nl{ate}, \\nl{the}, \\nl{cheese}).\\]Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.Since 2003, two other key developments in neural language modeling include:Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train.Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made “large enough” for many applications (GPT-3 used \\(n = 2048\\)).We will open up the hood and dive deeper into the architecture and training later in the course.\n",
      "SummaryLanguage models were first studied in the context of information theory, and can be used to estimate the entropy of English.N-gram models are extremely computationally efficient and statistically inefficient.N-gram models are useful for short context lengths in conjunction with another model (acoustic model for speech recognition or translation model for machine translation).Neural language models are statistically efficient but computationally inefficient.Over time, training large neural networks has become feasible enough that neural language models have become the dominant paradigm.\n",
      "Why does this course exist?Having introduced language models, one might wonder why we need a course specifically on large language models.Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa.\n",
      "CapabilitiesWhereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.Recall that language models are capable of conditional generation: given a prompt, generate a completion:\\[\\text{prompt} \\generate{} \\text{completion}.\\]Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):\\[\\nl{Frederic}, \\nl{Chopin}, \\nl{was}, \\nl{born}, \\nl{in} \\generate{T=0} \\nl{1810}, \\nl{in}, \\nl{Poland}\\]One can prompt a language model to solve word analogies (demo):\\[\\nl{sky}, \\nl{:}, \\nl{blue}, \\nl{::}, \\nl{grass}, \\nl{:} \\generate{T=0} \\nl{green}\\]One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. “It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt. “At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes.In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let’s start with an example (demo):Input: Where is Stanford University? Output: Stanford University is in California.We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):Input: Where is MIT? Output: Cambridge\n",
      "Input: Where is University of Washington? Output: Seattle\n",
      "Input: Where is Stanford University? Output: StanfordRelationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity.\n",
      "Language models in the real-worldGiven the strong capabilities of language models, it is not surprising to see their widespread adoption.Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:Google SearchFacebook content moderationMicrosoft’s Azure OpenAI ServiceAI21 Labs’ writing assistanceGiven the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.\n",
      "RisksSo far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind’s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)Input: Who invented the Internet? Output: Al GoreIn high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):The software developer finished the program. He celebrated. The software developer finished the program. She celebrated.Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content. For example:So, I’m starting to think she’s full _As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:Two Muslims walked into a _Applications such as writing assistants or chatbots would be vulnerable.Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:\\[\\nl{... Apple iPhone ...} \\generate{} \\text{(negative sentiment sentence)}.\\]In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?For example, if you prompt GPT-3 with the first line of Harry Potter (demo):Mr. and Mrs. Dursley of number four, Privet Drive, _It will happily continue to spout out text from Harry Potter with high confidence.Cost and environmental impact. Finally, large language models can be quite expensive to work with.Training often requires parallelizing over thousands of GPUs. For example, GPT-3 is estimated to cost around $5 million. This is a one-time cost.Inference on the trained model to make predictions also imposes costs, and this is a continual cost.One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project, EleutherAI, and Stanford’s CRFM. Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.\n",
      "SummaryA single large language model is a jack of all trades (and also master of none). It can perform a wide range of tasks and is capable of emergent behavior such as in-context learning.They are widely deployed in the real-world.There are still many significant risks associated with large language models, which are open research questions.Costs are a huge barrier for having broad access.\n",
      "Structure of this courseThis course will be structured like an onion:Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level.Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don’t have full access to the model.Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models.\n",
      "Further readingDan Jurafsky’s book on language modelsCS224N lecture notes on language modelsExploring the Limits of Language Modeling. R. Józefowicz, Oriol Vinyals, M. Schuster, Noam M. Shazeer, Yonghui Wu. 2016.On the Opportunities and Risks of Foundation Models. Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, E. Brynjolfsson, S. Buch, D. Card, Rodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S. Ermon, J. Etchemendy, Kawin Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, A. Narayan, D. Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, H. Nilforoshan, J. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, J. Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jackson K. Ryan, Christopher R’e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang. 2021.On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell. FAccT 2021.Ethical and social risks of harm from Language Models. Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sasha Brown, W. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel. 2021.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://stanford-cs324.github.io/winter2022/lectures/introduction\"\n",
    "html = urlopen(url).read()\n",
    "soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_lecture(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def preprocess_lecture(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Extract text content\n",
    "    text_content = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Extract tables\n",
    "    tables = []\n",
    "    for table in soup.find_all('table'):\n",
    "        headers = table.find_all(\"th\")\n",
    "        titles = []\n",
    "        for i in headers:\n",
    "            title = i.text\n",
    "            titles.append(title)\n",
    "        rows = table.find_all(\"tr\")\n",
    "        rows_data = []\n",
    "        for i in rows:\n",
    "            data = i.text\n",
    "            rows_data.append(data)\n",
    "        table_data = []\n",
    "        table_data.append(headers)\n",
    "        table_data.append(rows_data)\n",
    "        tables.append(table_data)\n",
    "    \n",
    "    # Extract images\n",
    "    images = [img['src'] for img in soup.find_all('img', src=True)]\n",
    "    \n",
    "    return {\n",
    "        'text': text_content,\n",
    "        'links': links,\n",
    "        'tables': tables,\n",
    "        'images': images\n",
    "    }\n",
    "\n",
    "lecture_urls = [\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/capabilities/\",\n",
    "    # Add more lecture URLs as needed\n",
    "]\n",
    "\n",
    "lectures_data = [preprocess_lecture(fetch_lecture(url)) for url in lecture_urls]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Capabilities | CS324 Link Search Menu Expand Document CS324 Home Calendar Lectures Introduction Capabilities Harms I Harms II Data Security Legality Modeling Training Parallelism Scaling laws Selective architectures Adaptation Environmental impact Paper reviews Paper discussions Projects This site uses Just the Docs , a documentation theme for Jekyll. Lectures Capabilities \\\\[\\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}} \\\\newcommand{\\\\perplexity}{\\\\text{perplexity}}\\\\] In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper , which include: standard NLP benchmarks (e.g., question answering), as well as quirky one-off demos (e.g., using a new word in a sentence). In comparison with the state-of-the-art-result for each task, the results are mixed : On some tasks such as language modeling, GPT-3 exceeds the state-of-the-art by a huge margin . On others, where GPT-3 is competing against systems that are trained with large amounts of labeled data, it lags far behind . The way to think about these results is as follows: GPT-3 was not trained on these tasks explicitly; it was just trained as a language model to predict the next word. Nonetheless, even without “trying” , GPT-3 does a passable job on average at a broad range of NLP tasks. Because GPT-3 was not trained on any of these tasks, it hasn’t overfit, which means it has a good chance of doing well at many many other tasks (as seen by the passable performance on one-off tasks). Moreover, if you wanted to do well on any particular task (e.g., question answering), you should in principle be able to adapt GPT-3 using the large amounts of labeled data to exceed state-of-the-art. Adaptation . Recall that a language model \\\\(p\\\\) is a distribution over sequences of tokens \\\\(x_{1:L}\\\\) and thus can be used to score sequences: \\\\[p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}).\\\\] It can also be used to perform conditional generation of a completion given a prompt: \\\\[\\\\nl{the mouse ate} \\\\generate{} \\\\nl{the cheese}.\\\\] A task is a mapping from inputs to outputs. For example, for question answering, we might have: Input: What school did burne hogarth establish? Output: School of Visual Arts We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given: a natural language description of the task, and a set of training instances (input-output pairs). There are two primary ways to perform adaptation: Training (standard supervised learning): train a new model that maps inputs to outputs, either by creating a new model that uses the language model as features (probing), or starting with the language model and updating it based on the training instances (fine-tuning), or something in between (lightweight fine-tuning). Prompting (in-context learning): Construct a prompt (a string based on the description and training instances) or a set of prompts, feed those into a language model to obtain completions. Zero-shot learning: number of training examples is 0 One-shot learning: number of training examples is 1 Few-shot learning: number of training examples is few Which adaptation procedure should we go with? Training can be challenging due to overfitting (just imagine fine-tuning a 175 billion parameter model based on 5 examples). How to do this effectively will be the topic of the adaptation lecture. For now, we will be content with adaptation of GPT-3 using prompting . Note that the limitation of prompting is that we can only leverage a only small number of training instances (as many as can fit into a prompt). This is due to a limitation of Transformers, where the prompt and the completion must fit into 2048 tokens. The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following: Definition : What is the task and its motivation? Adaptation : How do we reduce the task to language modeling (via prompting)? Results : What are the quantitative numbers compared to task-specific state-of-the-art models? Size and number of examples matters . By default, the results will based on the full GPT-3 model (davinci), which has 175 billion parameters using in-context learning with as many training instances as you can stuff into the prompt. Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better. The tasks are grouped as follows: Language modeling Question answering Translation Arithmetic News article generation Novel tasks The goals of this lecture is to provide: an overview of tasks in NLP (independent of large language models), a sense of how well GPT-3 works, and a taste for the art of prompt engineering. Language modeling The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\\\(p\\\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\\\(x_{1:L}\\\\), for example: \\\\[\\\\nl{the mouse ate the cheese}\\\\] We can ask: what is the probability the language model assigns to it? \\\\[p(\\\\nl{the mouse ate the cheese})\\\\] Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: \\\\[p(x_{1:L}) = \\\\prod_{i=1}^L p(x_i \\\\mid x_{1:i-1}).\\\\] Perplexity . The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\\\(p(x_i \\\\mid x_{1:i-1})\\\\). We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that. Instead, we want the geometric average , which is exactly what perplexity does: \\\\[\\\\perplexity_p(x_{1:L}) = \\\\exp\\\\left(\\\\frac{1}{L} \\\\sum_{i=1}^L \\\\log \\\\frac{1}{p(x_i \\\\mid x_{1:i-1})}\\\\right).\\\\] Perplexity can be interpreted as the average “branching factor” per token. Recall that \\\\(\\\\log \\\\frac{1}{p(x_i \\\\mid x_{1:i-1})}\\\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\\\(2^3\\\\) possible strings. Tale of two errors . There are two types of errors a language model can make, and perplexity treats them asymmetrically: Recall error : The language model fails to place probability mass on some token. Perplexity has no mercy: \\\\[p(\\\\nl{ate} \\\\mid \\\\nl{the}, \\\\nl{mouse}) \\\\to 0 \\\\quad\\\\Rightarrow\\\\quad \\\\perplexity_p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) \\\\to \\\\infty.\\\\] Precision error : The language model places extra probability mass on some bad sequences. Perplexity provides a slap on the wrist. Given a language model \\\\(p\\\\), suppose we mix in some garbage distribution \\\\(r\\\\) with probability \\\\(\\\\epsilon\\\\): \\\\[q(x_i \\\\mid x_{1:i-1}) = (1-\\\\epsilon) p(x_i \\\\mid x_{1:i-1}) + \\\\epsilon r(x_i \\\\mid x_{1:i-1}).\\\\] Then we can compute the perplexity of \\\\(x_{1:L}\\\\) under \\\\(q\\\\): \\\\[\\\\perplexity_q(x_{1:L}) \\\\le \\\\frac{1}{1 - \\\\epsilon} \\\\perplexity_p(x_{1:L}) \\\\approxeq (1 + \\\\epsilon) \\\\perplexity_p(x_{1:L}),\\\\] where the last approximate equality holds for small values of \\\\(\\\\epsilon\\\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token. Now let’s get on with evaluating perplexity on an actual dataset. Penn Tree Bank The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012) , a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out). Adaptation . Feed the entire text as a prompt into GPT-3 and evaluate the perplexity ( demo ): Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. Results . GPT-3 vastly outperforms the existing state-of-the-art: Model Perplexity GPT-3 20.5 BERT-Large-CAs1 31.3 See the leaderboard for the latest results. Train/test leakage . The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. LAMBADA ( Paperno et al. 2016 ) Task: predict the last word of a sentence. Motivation: Solving the task requires modeling long-range dependencies . Adaptation . LAMBADA is natively already a language modeling task, so we could just ask a language model to complete the final word of the sentence. Problem: language model doesn’t know it should be producing the final word of the sentence. Solution: frame it more explicitly as a input-output mapping and use in-context learning with additional examples ( demo ): Fill in blank: Alice was friends with Bob. Alice went to visit her friend ___. -> Bob She held the torch in front of her. She caught her breath. “Chris? There’s a step.” “What?” “A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step Results . GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2): Model Perplexity GPT-3 (few-shot) 1.92 SOTA 8.63 See the leaderboard for the latest results. HellaSwag ( Zellers et al. 2019 ) Motivation: evaluate a model’s ability to perform commonsense reasoning Task: choose the most appropriate completion for a sentence from a list of choices Adaptation . This is a multiple-choice task , so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one ( demo ): Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer} where ${answer} is one of: bake them, then frost and decorate. taste them as they place them on plates. put the frosting on the cake as they pan it. come out and begin decorating the cake as well. How do you score a candidate answer \\\\(y\\\\) given a question \\\\(x\\\\)? There’s no principled answer, but here are some heuristics : Unnormalized probability: \\\\(\\\\text{score}(x, y) = p(x, y)\\\\). The problem with the unnormalized probability is that it has a bias towards short answers ( demo ). Length-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(x, y)}{\\\\text{num-tokens}(y)}\\\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity. Frequency-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(y \\\\mid x)}{p(y \\\\mid x_0)}\\\\), where \\\\(x_0\\\\) is a neutral string like \\\\(\\\\nl{Answer:}\\\\). This lowers the score for answers that happen to just be common (e.g., \\\\nl{John}). Compare demo versus demo . Results . GPT-3 got close but did not exceed the state-of-the-art: Model Accuracy SOTA 85.6 GPT-3 79.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See the leaderboard for the latest results. Question answering Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided). Input: What school did burne hogarth establish? Output: School of Visual Arts TriviaQA ( Joshi et al. 2017 ) Task: given a trivia question, generate the answer The original dataset was collected from trivial enthusiasts and was presented as a challenge used for (open book) reading comprehension, but we use it for (closed-book) question answering. Adaptation . We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer ( demo ): Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Marcel Duchamp Results . Model Accuracy RAG 68.0 GPT-3 (zero-shot) 64.3 GPT-3 (few-shot) 71.2 We also see that both increasing the model size and the number of in-context training instances helps: WebQuestions ( Berant et al. 2013 ) Task: answer questions Dataset collected from Google search queries, initially created for question answering on knowledge bases Adaptation . We define a prompt the same as above ( demo ): Q: What school did burne hogarth establish? A: School of Visual Arts Results . Model Accuracy RAG 45.5 GPT-3 (zero-shot) 14.4 GPT-3 (few-shot) 41.5 NaturalQuestions Task: answer questions Dataset collected from Google search queries (with long-form answers) Adaptation . We define a prompt the same as above ( demo ): Q: Who played tess on touched by an angel? A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese. Results . Model Accuracy RAG 44.5 GPT-3 (zero-shot) 14.6 GPT-3 (few-shot) 29.9 Translation Task: translate a sentence in a source language (e.g., German) to sentence in a target language (e.g., English) Machine translation has been a long standing NLP task since the 1960s, and statistical machine translation took off within NLP (with its own distinct subcommunity) in the 2000s, followed by neural machine translation in the mid-2010s. It has always been a data-rich field due to the existence of human translators. The standard evaluation dataset is the WMT’14 and WMT’16 datasets. Since there are multiple possible translations, the (automatic) evaluation metric is BLEU (which captures a notion of n-gram overlap). Adaptation . For the few-shot setting, we construct a prompt containing input-output training instances along with the input ( demo ): Mein Haus liegt auf dem Hügel. = My house is on the hill. Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. = In no case may they be used for commercial purposes. Results . Here are the results from German to English: Model Accuracy SOTA (supervised) 40.2 GPT-3 (zero-shot) 27.2 GPT-3 (few-shot) 40.6 Even without supervised training data, GPT-3 matches the state-of-the-art of a fully-supervised system! This presents a lower bound on how well one can do in machine translation; you would definitely want to leverage the large amount of parallel corpora (aligned input-output pairs). Results from French and Romanian are similar. Results from English to a foreign language is much worse, which is expected since GPT-3 is primarily an English language model. Arithmetic GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model. Task: do arithmetic (2-5 digit addition, subtraction, multiplication) There’s no practical reason you would want to solve this task; it’s just a diagnostic task to satisfy our scientific curiosity. Adaptation . Pose the problem as question answering ( demo ): Q: What is 556 plus 497? A: 1053 Results . It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well. News article generation Task: given title and subtitle, generate a news article Dataset: title/subtitles taken from newser.com Evaluation: humans rated articles based on how likely the article was likely to be written by a machine Adaptation . Note: in-context learning was needed to give the model an idea of what a prompt looks like. Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination... Results . Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance). For the article above, humans guessed “machine” correctly only 12% of the time. Novel tasks Using new words Task: given a new made-up word and a definition, generate a sentence that uses the word. Adaptation . Just describe the task in the prompt ( demo ): To “screeg” something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeged the tree with our swords. Correcting English grammar Task: given an ungrammatical sentence, generate its grammatical version. Adaptation . The prompt consists of input-output pairs ( demo ): Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I would be happy to work with you on another project. Other tasks Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks . SWORDS : lexical substitution, where the goal is to predict synonyms in the context of a sentence. Massive Multitask Language Understanding : 57 multiple-choice problems spanning mathematics, US history, computer science, law, etc. TruthfulQA : question answering dataset that humans would answer falsely due to misconceptions. The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning! Demos . Examples from the OpenAI website Examples from gpt3demo.com The demos are creative and interesting, but it’s hard to tell how reliably they work. Summary GPT-3 was evaluated on a wide range of standard NLP benchmarks and on quirky one-off tasks. GPT-3 can perform extremely well or be very medicore. Both increasing the size of the model and the number of examples helps performance. There are a few heuristic ways of adapting the language model to the task of interest. Why does this work? No one knows. Further reading Language Models are Few-Shot Learners . Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei . NeurIPS 2020. Blog post explaining perplexity',\n",
       "  'links': ['https://stanford-cs324.github.io/winter2022/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/',\n",
       "   'https://stanford-cs324.github.io/winter2022/calendar/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/introduction/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/capabilities/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-1/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-2/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/data/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/security/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/legality/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/modeling/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/training/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/parallelism/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/adaptation/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/environment/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-reviews/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-discussions/',\n",
       "   'https://stanford-cs324.github.io/winter2022/projects/',\n",
       "   'https://github.com/pmarsceill/just-the-docs',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   'https://arxiv.org/pdf/2005.14165.pdf',\n",
       "   '#language-modeling',\n",
       "   '#question-answering',\n",
       "   '#translation',\n",
       "   '#arithmetic',\n",
       "   '#news-article-generation',\n",
       "   '#novel-tasks',\n",
       "   '#language-modeling',\n",
       "   '#penn-tree-bank',\n",
       "   'https://catalog.ldc.upenn.edu/LDC99T42',\n",
       "   'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1325968',\n",
       "   'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6424228',\n",
       "   'https://nlp.stanford.edu/~johnhew/',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Pierre%20Vinken%2C%2061%20years%20old%2C%20will%20join%20the%20board%20as%20a%20nonexecutive%20director%20Nov.%2029.%20%20Mr.%20Vinken%20is%20chairman%20of%20Elsevier%20N.V.%2C%20the%20Dutch%20publishing%20group.&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200%0Atop_k_per_token%3A%205%0Amodel%3A%20%24%7Bmodel%7D&environments=model%3A%20%5Bopenai%2Fdavinci%2C%20openai%2Fcurie%2C%20ai21%2Fj1-jumbo%5D',\n",
       "   'https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word',\n",
       "   'https://paperswithcode.com/dataset/wikitext-103',\n",
       "   '#lambada-paperno-et-al-2016',\n",
       "   'https://arxiv.org/pdf/1606.06031.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Fill%20in%20blank%3A%0A%0AAlice%20was%20friends%20with%20Bob.%20Alice%20went%20to%20visit%20her%20friend%20___.%20-%3E%20Bob%0A%0AShe%20held%20the%20torch%20in%20front%20of%20her.%0AShe%20caught%20her%20breath.%0A%22Chris%3F%20%20There%E2%80%99s%20a%20step.%22%0A%22What%3F%22%0A%22A%20step.%20Cut%20in%20the%20rock.%20About%20fifty%20feet%20ahead.%22%20She%20moved%20faster.%20They%20both%20moved%20faster.%20%22In%20fact%2C%22%20she%20said%2C%20raising%20the%20torch%20higher%2C%20%22there%E2%80%99s%20more%20than%20a%20___.%20-%3E&settings=temperature%3A%200%0Amax_tokens%3A%201%0Atop_k_per_token%3A%2010%0Amodel%3A%20%24%7Bmodel%7D&environments=model%3A%20%5Bopenai%2Fdavinci%2C%20openai%2Fcurie%2C%20ai21%2Fj1-jumbo%5D',\n",
       "   'https://paperswithcode.com/sota/language-modelling-on-lambada',\n",
       "   '#hellaswag-zellers-et-al-2019',\n",
       "   'https://arxiv.org/pdf/1905.07830.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Making%20a%20cake%3A%20Several%20cake%20pops%20are%20shown%20on%20a%20display.%20%20A%20woman%20and%20girl%20are%20shown%20making%20the%20cake%20pops%20in%20a%20kitchen.%20%0A%20They%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%205%0Aecho_prompt%3A%20true&environments=answer%3A%20%5B%22bake%20them%2C%20then%20frost%20and%20decorate.%22%2C%20%22taste%20them%20as%20they%20place%20them%20on%20plates.%22%2C%20%22put%20the%20frosting%20on%20the%20cake%20as%20they%20pan%20it.%22%2C%20%22come%20out%20and%20begin%20decorating%20the%20cake%20as%20well.%22%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=Question%3A%20Why%20is%20the%20sky%20blue%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BIt%27s%20due%20to%20a%20phenomenon%20called%20Raleigh%20scattering%2C%20Because%20Mars%20is%20red%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=John%20pushed%20Khaleesi.%0AQuestion%3A%20Who%20was%20upset%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BJohn%2C%20Khaleesi%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=John%20pushed%20Bob.%0AQuestion%3A%20Who%20was%20upset%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BJohn%2C%20Bob%5D',\n",
       "   'https://paperswithcode.com/sota/sentence-completion-on-hellaswag',\n",
       "   '#question-answering',\n",
       "   '#triviaqa-joshi-et-al-2017',\n",
       "   'https://arxiv.org/pdf/1705.03551.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20%E2%80%98Nude%20Descending%20A%20Staircase%E2%80%99%20is%20perhaps%20the%20most%20famous%20painting%20by%20which%0A20th%20century%20artist%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#webquestions-berant-et-al-2013',\n",
       "   'https://aclanthology.org/D13-1160.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20What%20school%20did%20burne%20hogarth%20establish%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#naturalquestions',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20Who%20played%20tess%20on%20touched%20by%20an%20angel%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#translation',\n",
       "   'https://paperswithcode.com/dataset/wmt-2014',\n",
       "   'https://paperswithcode.com/dataset/wmt-2016',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Mein%20Haus%20liegt%20auf%20dem%20H%C3%BCgel.%20%3D%20My%20house%20is%20on%20the%20hill.%0AKeinesfalls%20d%C3%BCrfen%20diese%20f%C3%BCr%20den%20kommerziellen%20Gebrauch%20verwendet%20werden.%20%3D&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#arithmetic',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20What%20is%20556%20plus%20497%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#news-article-generation',\n",
       "   'newser.com',\n",
       "   '#novel-tasks',\n",
       "   '#using-new-words',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=To%20%22screeg%22%20something%20is%20to%20swing%20a%20sword%20at%20it.%20%20An%20example%20of%20a%20sentence%20that%20uses%20the%20word%20screeg%20is%3A%0AWe&settings=stop_sequences%3A%20%5B%5Cn%5D%0Anum_completions%3A%2010%0Atemperature%3A%200.5&environments=',\n",
       "   '#correcting-english-grammar',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Poor%20English%20input%3A%20I%20eated%20the%20purple%20berries.%0AGood%20English%20output%3A%20I%20ate%20the%20purple%20berries.%0APoor%20English%20input%3A%20Thank%20you%20for%20picking%20me%20as%20your%20designer.%20I%E2%80%99d%20appreciate%20it.%0AGood%20English%20output%3A%20Thank%20you%20for%20choosing%20me%20as%20your%20designer.%20I%20appreciate%20it.%0APoor%20English%20input%3A%20The%20mentioned%20changes%20have%20done.%20or%20I%20did%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20did%20the%20modifications.%0AGood%20English%20output%3A%20The%20requested%20changes%20have%20been%20made.%20or%20I%20made%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20made%20the%20modifications.%0APoor%20English%20input%3A%20I%E2%80%99d%20be%20more%20than%20happy%20to%20work%20with%20you%20in%20another%20project.%0AGood%20English%20output%3A&settings=stop_sequences%3A%20%5B%5Cn%5D%0Atemperature%3A%200%0Atop_k_per_token%3A%205&environments=',\n",
       "   '#other-tasks',\n",
       "   'https://arxiv.org/pdf/2106.04102.pdf',\n",
       "   'https://arxiv.org/pdf/2009.03300.pdf',\n",
       "   'https://arxiv.org/pdf/2109.07958.pdf',\n",
       "   'https://beta.openai.com/examples/',\n",
       "   'https://gpt3demo.com/',\n",
       "   '#summary',\n",
       "   '#further-reading',\n",
       "   'https://arxiv.org/pdf/2005.14165.pdf',\n",
       "   'https://towardsdatascience.com/perplexity-in-language-models-87a196019a94'],\n",
       "  'tables': [[[],\n",
       "    ['ModelPerplexityGPT-320.5BERT-Large-CAs131.3',\n",
       "     'GPT-320.5BERT-Large-CAs131.3',\n",
       "     'BERT-Large-CAs131.3']],\n",
       "   [[],\n",
       "    ['ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63',\n",
       "     'GPT-3 (few-shot)1.92SOTA8.63',\n",
       "     'SOTA8.63']],\n",
       "   [[], ['ModelAccuracySOTA85.6GPT-379.3', 'SOTA85.6GPT-379.3', 'GPT-379.3']],\n",
       "   [[],\n",
       "    ['ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'RAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (few-shot)71.2']],\n",
       "   [[],\n",
       "    ['ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'RAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (few-shot)41.5']],\n",
       "   [[],\n",
       "    ['ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'RAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (few-shot)29.9']],\n",
       "   [[],\n",
       "    ['ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'SOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (few-shot)40.6']]],\n",
       "  'images': ['../images/gpt3-trivia-qa-performance.png',\n",
       "   '../images/gpt3-arithmetic-performance.png']}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_lecture(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def preprocess_lecture(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Extract text content\n",
    "    text_content = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Extract tables\n",
    "    tables = []\n",
    "    for table in soup.find_all('table'):\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            row_data = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "            table_data.append(row_data)\n",
    "        tables.append(table_data)\n",
    "    \n",
    "    # Extract images\n",
    "    images = [img['src'] for img in soup.find_all('img', src=True)]\n",
    "    \n",
    "    return {\n",
    "        'text': text_content,\n",
    "        'links': links,\n",
    "        'tables': tables,\n",
    "        'images': images\n",
    "    }\n",
    "\n",
    "lecture_urls = [\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/introduction/\",\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/capabilities/\",\n",
    "    # Add more lecture URLs as needed\n",
    "]\n",
    "\n",
    "lectures_data = [preprocess_lecture(fetch_lecture(url)) for url in lecture_urls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Introduction | CS324 Link Search Menu Expand Document CS324 Home Calendar Lectures Introduction Capabilities Harms I Harms II Data Security Legality Modeling Training Parallelism Scaling laws Selective architectures Adaptation Environmental impact Paper reviews Paper discussions Projects This site uses Just the Docs , a documentation theme for Jekyll. Lectures Introduction \\\\[\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}}\\\\] Welcome to CS324! This is a new course on understanding and developing large language models . What is a language model? A brief history Why does this course exist? Structure of this course What is a language model? The classic definition of a language model (LM) is a probability distribution over sequences of tokens . Suppose we have a vocabulary \\\\(\\\\sV\\\\) of a set of tokens. A language model \\\\(p\\\\) assigns each sequence of tokens \\\\(x_1, \\\\dots, x_L \\\\in \\\\sV\\\\) a probability (a number between 0 and 1): \\\\[p(x_1, \\\\dots, x_L).\\\\] The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\\\(\\\\sV = \\\\{ \\\\nl{ate}, \\\\nl{ball}, \\\\nl{cheese}, \\\\nl{mouse}, \\\\nl{the} \\\\}\\\\), the language model might assign ( demo ): \\\\[p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) = 0.02,\\\\] \\\\[p(\\\\nl{the}, \\\\nl{cheese}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{mouse}) = 0.01,\\\\] \\\\[p(\\\\nl{mouse}, \\\\nl{the}, \\\\nl{the}, \\\\nl{cheese}, \\\\nl{ate}) = 0.0001.\\\\] Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit ) linguistic abilities and world knowledge. For example, the LM should assign \\\\(\\\\nl{mouse the the cheese ate}\\\\) a very low probability implicitly because it’s ungrammatical ( syntactic knowledge ). The LM should assign \\\\(\\\\nl{the mouse ate the cheese}\\\\) higher probability than \\\\(\\\\nl{the cheese ate the mouse}\\\\) implicitly because of world knowledge : both sentences are the same syntactically, but they differ in semantic plausibility. Generation . As defined, a language model \\\\(p\\\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\\\(x_{1:L}\\\\) from the language model \\\\(p\\\\) with probability equal to \\\\(p(x_{1:L})\\\\), denoted: \\\\[x_{1:L} \\\\sim p.\\\\] How to do this computationally efficiently depends on the form of the language model \\\\(p\\\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence. Autoregressive language models A common way to write the joint distribution \\\\(p(x_{1:L})\\\\) of a sequence \\\\(x_{1:L}\\\\) is using the chain rule of probability : \\\\[p(x_{1:L}) = p(x_1) p(x_2 \\\\mid x_1) p(x_3 \\\\mid x_1, x_2) \\\\cdots p(x_L \\\\mid x_{1:L-1}) = \\\\prod_{i=1}^L p(x_i \\\\mid x_{1:i-1}).\\\\] For example ( demo ): \\\\[\\\\begin{align*} p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) = \\\\, & p(\\\\nl{the}) \\\\\\\\ & p(\\\\nl{mouse} \\\\mid \\\\nl{the}) \\\\\\\\ & p(\\\\nl{ate} \\\\mid \\\\nl{the}, \\\\nl{mouse}) \\\\\\\\ & p(\\\\nl{the} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}) \\\\\\\\ & p(\\\\nl{cheese} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}). \\\\end{align*}\\\\] In particular, \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) is a conditional probability distribution of the next token \\\\(x_i\\\\) given the previous tokens \\\\(x_{1:i-1}\\\\). Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) can be computed efficiently (e.g., using a feedforward neural network). Generation . Now to generate an entire sequence \\\\(x_{1:L}\\\\) from an autoregressive language model \\\\(p\\\\), we sample one token at a time given the tokens generated so far: \\\\[\\\\text{for } i = 1, \\\\dots, L: \\\\\\\\ \\\\hspace{1in} x_i \\\\sim p(x_i \\\\mid x_{1:i-1})^{1/T},\\\\] where \\\\(T \\\\ge 0\\\\) is a temperature parameter that controls how much randomness we want from the language model: \\\\(T = 0\\\\): deterministically choose the most probable token \\\\(x_i\\\\) at each position \\\\(i\\\\) \\\\(T = 1\\\\): sample “normally” from the pure language model \\\\(T = \\\\infty\\\\): sample from a uniform distribution over the entire vocabulary \\\\(\\\\sV\\\\) However, if we just raise the probabilities to the power \\\\(1/T\\\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\\\(p_T(x_i \\\\mid x_{1:i-1}) \\\\propto p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) the annealed conditional probability distribution. For example: \\\\[p(\\\\nl{cheese}) = 0.4, \\\\quad\\\\quad\\\\quad p(\\\\nl{mouse}) = 0.6\\\\] \\\\[p_{T=0.5}(\\\\nl{cheese}) = 0.31, \\\\quad\\\\quad\\\\quad p_{T=0.5}(\\\\nl{mouse}) = 0.69\\\\] \\\\[p_{T=0.2}(\\\\nl{cheese}) = 0.12, \\\\quad\\\\quad\\\\quad p_{T=0.2}(\\\\nl{mouse}) = 0.88\\\\] \\\\[p_{T=0}(\\\\nl{cheese}) = 0, \\\\quad\\\\quad\\\\quad p_{T=0}(\\\\nl{mouse}) = 1\\\\] Aside : Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note : sampling iteratively with a temperature \\\\(T\\\\) parameter applied to each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) is not equivalent (except when \\\\(T = 1\\\\)) to sampling from the annealed distribution over length \\\\(L\\\\) sequences. Conditional generation . More generally, we can perform conditional generation by specifying some prefix sequence \\\\(x_{1:i}\\\\) (called a prompt ) and sampling the rest \\\\(x_{i+1:L}\\\\) (called the completion ). For example, generating with \\\\(T=0\\\\) produces ( demo ): \\\\[\\\\underbrace{\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}}_\\\\text{prompt} \\\\generate{T=0} \\\\underbrace{\\\\nl{the}, \\\\nl{cheese}}_\\\\text{completion}.\\\\] If we change the temperature to \\\\(T = 1\\\\), we can get more variety ( demo ), for example, \\\\(\\\\nl{its house}\\\\) and \\\\(\\\\nl{my homework}\\\\). As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt. Summary A language model is a probability distribution \\\\(p\\\\) over sequences \\\\(x_{1:L}\\\\). Intuitively, a good language model should have linguistic capabilities and world knowledge. An autoregressive language model allows for efficient generation of a completion \\\\(x_{i+1:L}\\\\) given a prompt \\\\(x_{1:i}\\\\). The temperature can be used to control the amount of variability in generation. A brief history Information theory, entropy of English, n-gram models Information theory . Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication . In this paper, he introduced the entropy of a distribution as \\\\[H(p) = \\\\sum_x p(x) \\\\log \\\\frac{1}{p(x)}.\\\\] The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\\\(x \\\\sim p\\\\) into a bitstring: \\\\[\\\\nl{the mouse ate the cheese} \\\\Rightarrow 0001110101.\\\\] The lower the entropy, the more “structured” the sequence is, and the shorter the code length. Intuitively, \\\\(\\\\log \\\\frac{1}{p(x)}\\\\) is the length of the code used to represent an element \\\\(x\\\\) that occurs with probability \\\\(p(x)\\\\). If \\\\(p(x) = \\\\frac{1}{8}\\\\), we should allocate \\\\(\\\\log_2(8) = 3\\\\) bits (equivalently, \\\\(\\\\log(8) = 2.08\\\\) nats). Aside : actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English . Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\\\(p\\\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\\\(x \\\\sim p\\\\). Shannon also defined cross entropy : \\\\[H(p, q) = \\\\sum_x p(x) \\\\log \\\\frac{1}{q(x)},\\\\] which measures the expected number of bits (nats) needed to encode a sample \\\\(x \\\\sim p\\\\) using the compression scheme given by the model \\\\(q\\\\) (representing \\\\(x\\\\) with a code of length \\\\(\\\\frac{1}{q(x)}\\\\)). Estimating entropy via language modeling . A crucial property is that the cross entropy \\\\(H(p, q)\\\\) upper bounds the entropy \\\\(H(p)\\\\), \\\\[H(p, q) \\\\ge H(p),\\\\] which means that we can estimate \\\\(H(p, q)\\\\) by constructing a (language) model \\\\(q\\\\) with only samples from the true data distribution \\\\(p\\\\), whereas \\\\(H(p)\\\\) is generally inaccessible if \\\\(p\\\\) is English. So we can get better estimates of the entropy \\\\(H(p)\\\\) by constructing better models \\\\(q\\\\), as measured by \\\\(H(p, q)\\\\). Shannon game (human language model) . Shannon first used n-gram models as \\\\(q\\\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English , he introduced a clever scheme (known as the Shannon game) where \\\\(q\\\\) was provided by a human: \\\\[\\\\nl{the mouse ate my ho_}\\\\] Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. N-gram models for downstream applications Language models became first used in practical applications that required generation of text: speech recognition in the 1970s (input: acoustic signal, output: text), and machine translation in the 1990s (input: text in a source language, output: text in a target language). Noisy channel model . The dominant paradigm for solving these tasks then was the noisy channel model . Taking speech recognition as an example: We posit that there is some text sampled from some distribution \\\\(p\\\\). This text becomes realized to speech (acoustic signals). Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule: \\\\[p(\\\\text{text} \\\\mid \\\\text{speech}) \\\\propto \\\\underbrace{p(\\\\text{text})}_\\\\text{language model} \\\\underbrace{p(\\\\text{speech} \\\\mid \\\\text{text})}_\\\\text{acoustic model}.\\\\] Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models . In an n-gram model , the prediction of a token \\\\(x_i\\\\) only depends on the last \\\\(n-1\\\\) characters \\\\(x_{i-(n-1):i-1}\\\\) rather than the full history: \\\\[p(x_i \\\\mid x_{1:i-1}) = p(x_i \\\\mid x_{i-(n-1):i-1}).\\\\] For example, a trigram (\\\\(n=3\\\\)) model would define: \\\\[p(\\\\nl{cheese} \\\\mid \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}) = p(\\\\nl{cheese} \\\\mid \\\\nl{ate}, \\\\nl{the}).\\\\] These probabilities are computed based on the number of times various n-grams (e.g., \\\\(\\\\nl{ate the mouse}\\\\) and \\\\(\\\\nl{ate the cheese}\\\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: \\\\[\\\\nl{Stanford has a new course on large language models. It will be taught by ___}\\\\] If \\\\(n\\\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\\\(\\\\nl{Stanford}\\\\). However, if \\\\(n\\\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora): \\\\[\\\\text{count}(\\\\nl{Stanford}, \\\\nl{has}, \\\\nl{a}, \\\\nl{new}, \\\\nl{course}, \\\\nl{on}, \\\\nl{large}, \\\\nl{language}, \\\\nl{models}) = 0.\\\\] As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem. Neural language models An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\\\(p(x_i \\\\mid x_{i-(n-1):i-1})\\\\) is given by a neural network: \\\\[p(\\\\nl{cheese} \\\\mid \\\\nl{ate}, \\\\nl{the}) = \\\\text{some-neural-network}(\\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}).\\\\] Note that the context length is still bounded by \\\\(n\\\\), but it is now statistically feasible to estimate neural language models for much larger values of \\\\(n\\\\). Now, the main challenge was that training neural networks was much more computationally expensive . They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\\\(x_i\\\\) to depend on the entire context \\\\(x_{1:i-1}\\\\) (effectively \\\\(n = \\\\infty\\\\)), but these were hard to train. Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\\\(n\\\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\\\(n\\\\) could be made “large enough” for many applications (GPT-3 used \\\\(n = 2048\\\\)). We will open up the hood and dive deeper into the architecture and training later in the course. Summary Language models were first studied in the context of information theory, and can be used to estimate the entropy of English. N-gram models are extremely computationally efficient and statistically inefficient. N-gram models are useful for short context lengths in conjunction with another model (acoustic model for speech recognition or translation model for machine translation). Neural language models are statistically efficient but computationally inefficient. Over time, training large neural networks has become feasible enough that neural language models have become the dominant paradigm. Why does this course exist? Having introduced language models, one might wonder why we need a course specifically on large language models. Increase in size . First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years: Model Organization Date Size (# params) ELMo AI2 Feb 2018 94,000,000 GPT OpenAI Jun 2018 110,000,000 BERT Google Oct 2018 340,000,000 XLM Facebook Jan 2019 655,000,000 GPT-2 OpenAI Mar 2019 1,500,000,000 RoBERTa Facebook Jul 2019 355,000,000 Megatron-LM NVIDIA Sep 2019 8,300,000,000 T5 Google Oct 2019 11,000,000,000 Turing-NLG Microsoft Feb 2020 17,000,000,000 GPT-3 OpenAI May 2020 175,000,000,000 Megatron-Turing NLG Microsoft, NVIDIA Oct 2021 530,000,000,000 Gopher DeepMind Dec 2021 280,000,000,000 Emergence . What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside : at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Capabilities Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable of conditional generation : given a prompt, generate a completion: \\\\[\\\\text{prompt} \\\\generate{} \\\\text{completion}.\\\\] Examples of capabilities . This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank ( demo ): \\\\[\\\\nl{Frederic}, \\\\nl{Chopin}, \\\\nl{was}, \\\\nl{born}, \\\\nl{in} \\\\generate{T=0} \\\\nl{1810}, \\\\nl{in}, \\\\nl{Poland}\\\\] One can prompt a language model to solve word analogies ( demo ): \\\\[\\\\nl{sky}, \\\\nl{:}, \\\\nl{blue}, \\\\nl{::}, \\\\nl{grass}, \\\\nl{:} \\\\generate{T=0} \\\\nl{green}\\\\] One can prompt a language model to generate a news article based on a headline ( demo ). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. “It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt. “At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes. In-context learning . Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning . Let’s start with an example ( demo ): Input: Where is Stanford University? Output: Stanford University is in California. We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer ( demo ): Input: Where is MIT? Output: Cambridge Input: Where is University of Washington? Output: Seattle Input: Where is Stanford University? Output: Stanford Relationship to supervised learning . In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior. Aside : neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Language models in the real-world Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research . First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry . In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Google Search Facebook content moderation Microsoft’s Azure OpenAI Service AI21 Labs’ writing assistance Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people . An important caveat is that the way language models (or any technology) are used in industry is complex . They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. Risks So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper , the foundation models report , and DeepMind’s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability . If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing ( demo ) Input: Who invented the Internet? Output: Al Gore In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias . It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun ( demo ): The software developer finished the program. He celebrated. The software developer finished the program. She celebrated. Social biases are of course encoded in the data , and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity . Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content. For example: So, I’m starting to think she’s full _ As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes : Two Muslims walked into a _ Applications such as writing assistants or chatbots would be vulnerable. Disinformation . We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security . Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\\\(\\\\nl{Apple iPhone}\\\\) is in the prompt: \\\\[\\\\nl{... Apple iPhone ...} \\\\generate{} \\\\text{(negative sentiment sentence)}.\\\\] In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations . Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter ( demo ): Mr. and Mrs. Dursley of number four, Privet Drive, _ It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact . Finally, large language models can be quite expensive to work with. Training often requires parallelizing over thousands of GPUs. For example, GPT-3 is estimated to cost around $5 million. This is a one-time cost. Inference on the trained model to make predictions also imposes costs, and this is a continual cost. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact . However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access . An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project , EleutherAI , and Stanford’s CRFM . Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology. Summary A single large language model is a jack of all trades (and also master of none). It can perform a wide range of tasks and is capable of emergent behavior such as in-context learning. They are widely deployed in the real-world. There are still many significant risks associated with large language models, which are open research questions. Costs are a huge barrier for having broad access. Structure of this course This course will be structured like an onion: Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level. Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don’t have full access to the model. Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.). Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models , which share many of the properties of language models. Further reading Dan Jurafsky’s book on language models CS224N lecture notes on language models Exploring the Limits of Language Modeling . R. Józefowicz, Oriol Vinyals, M. Schuster, Noam M. Shazeer, Yonghui Wu . 2016. On the Opportunities and Risks of Foundation Models . Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, E. Brynjolfsson, S. Buch, D. Card, Rodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S. Ermon, J. Etchemendy, Kawin Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, A. Narayan, D. Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, H. Nilforoshan, J. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, J. Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jackson K. Ryan, Christopher R’e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang . 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 . Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell . FAccT 2021. Ethical and social risks of harm from Language Models . Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sasha Brown, W. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel . 2021.',\n",
       "  'links': ['https://stanford-cs324.github.io/winter2022/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/',\n",
       "   'https://stanford-cs324.github.io/winter2022/calendar/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/introduction/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/capabilities/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-1/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-2/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/data/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/security/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/legality/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/modeling/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/training/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/parallelism/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/adaptation/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/environment/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-reviews/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-discussions/',\n",
       "   'https://stanford-cs324.github.io/winter2022/projects/',\n",
       "   'https://github.com/pmarsceill/just-the-docs',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   '#what-is-a-language-model',\n",
       "   '#a-brief-history',\n",
       "   '#why-does-this-course-exist',\n",
       "   '#structure-of-this-course',\n",
       "   '#what-is-a-language-model',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=%24%7Bprompt%7D&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200&environments=prompt%3A%20%5Bthe%20mouse%20ate%20the%20cheese%2C%20the%20cheese%20ate%20the%20mouse%2C%20mouse%20the%20the%20cheese%20ate%5D',\n",
       "   '#autoregressive-language-models',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate%20the%20cheese&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%200%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%201%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=',\n",
       "   '#summary',\n",
       "   '#a-brief-history',\n",
       "   '#information-theory-entropy-of-english-n-gram-models',\n",
       "   'https://dl.acm.org/doi/pdf/10.1145/584091.584093',\n",
       "   'https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6773263',\n",
       "   '#n-gram-models-for-downstream-applications',\n",
       "   'https://aclanthology.org/D07-1090.pdf',\n",
       "   '#neural-language-models',\n",
       "   'https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf',\n",
       "   '#summary-1',\n",
       "   '#why-does-this-course-exist',\n",
       "   '#capabilities',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Frederic%20Chopin%20was%20born%20in&settings=temperature%3A%200%0Astop_sequences%3A%20%5B.%5D%0Atop_k_per_token%3A%205&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=sky%20%3A%20blue%20%3A%3A%20grass%20%3A&settings=temperature%3A%200%20%20%23%20Deterministic%0Amax_tokens%3A%201%0Atop_k_per_token%3A%205&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Title%3A%20New%20Course%20on%20Understanding%20and%20Developing%20Large%20Language%20Models%20(CS324)%0AArticle%3A%20On%20January%203%2C&settings=temperature%3A%200.5%0Amax_tokens%3A%20200%0Atop_k_per_token%3A%205&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Input%3A%20Where%20is%20Stanford%20University%3F%0AOutput%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D%0Atop_k_per_token%3A%205&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Input%3A%20Where%20is%20MIT%3F%0AOutput%3A%20Cambridge%0A%0AInput%3A%20Where%20is%20University%20of%20Washington%3F%0AOutput%3A%20Seattle%0A%0AInput%3A%20Where%20is%20Stanford%20University%3F%0AOutput%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D%0Atop_k_per_token%3A%205&environments=',\n",
       "   '#language-models-in-the-real-world',\n",
       "   'https://blog.google/products/search/search-language-understanding-bert/',\n",
       "   'https://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/',\n",
       "   'https://blogs.microsoft.com/ai/new-azure-openai-service/',\n",
       "   'https://www.ai21.com/',\n",
       "   '#risks',\n",
       "   'https://dl.acm.org/doi/pdf/10.1145/3442188.3445922',\n",
       "   'https://arxiv.org/pdf/2108.07258.pdf',\n",
       "   'https://arxiv.org/pdf/2112.04359.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Input%3A%20Who%20invented%20the%20Internet%3F%0AOutput%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D%0Atop_k_per_token%3A%205&environments=',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=The%20software%20developer%20finished%20the%20program.%20%20%24%7Bpronoun%7D%20celebrated.&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200&environments=pronoun%3A%20%5BHe%2C%20She%5D',\n",
       "   'https://arxiv.org/pdf/2009.11462.pdf',\n",
       "   'https://arxiv.org/pdf/2101.05783.pdf',\n",
       "   'https://arxiv.org/pdf/2010.12563.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Mr.%20and%20Mrs.%20Dursley%20of%20number%20four%2C%20Privet%20Drive%2C&settings=temperature%3A%200%0Atop_k_per_token%3A%205&environments=',\n",
       "   'https://bigscience.huggingface.co/',\n",
       "   'https://www.eleuther.ai/',\n",
       "   'https://crfm.stanford.edu/',\n",
       "   '#summary-2',\n",
       "   '#structure-of-this-course',\n",
       "   'https://arxiv.org/pdf/2108.07258.pdf',\n",
       "   '#further-reading',\n",
       "   'https://web.stanford.edu/~jurafsky/slp3/3.pdf',\n",
       "   'https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf',\n",
       "   'https://arxiv.org/pdf/1602.02410.pdf',\n",
       "   'https://arxiv.org/pdf/2108.07258.pdf',\n",
       "   'https://dl.acm.org/doi/pdf/10.1145/3442188.3445922',\n",
       "   'https://arxiv.org/pdf/2112.04359.pdf'],\n",
       "  'tables': [[['ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Size (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'AI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '94,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'AI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '94,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Mar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '1,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'FacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Jul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'NVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Sep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '8,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '11,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'MicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Feb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '17,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'May 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Microsoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'Oct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     '530,000,000,000GopherDeepMindDec 2021280,000,000,000',\n",
       "     'GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000'],\n",
       "    ['GopherDeepMindDec 2021280,000,000,000',\n",
       "     'DeepMindDec 2021280,000,000,000',\n",
       "     'Dec 2021280,000,000,000',\n",
       "     '280,000,000,000']]],\n",
       "  'images': []},\n",
       " {'text': 'Capabilities | CS324 Link Search Menu Expand Document CS324 Home Calendar Lectures Introduction Capabilities Harms I Harms II Data Security Legality Modeling Training Parallelism Scaling laws Selective architectures Adaptation Environmental impact Paper reviews Paper discussions Projects This site uses Just the Docs , a documentation theme for Jekyll. Lectures Capabilities \\\\[\\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}} \\\\newcommand{\\\\perplexity}{\\\\text{perplexity}}\\\\] In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper , which include: standard NLP benchmarks (e.g., question answering), as well as quirky one-off demos (e.g., using a new word in a sentence). In comparison with the state-of-the-art-result for each task, the results are mixed : On some tasks such as language modeling, GPT-3 exceeds the state-of-the-art by a huge margin . On others, where GPT-3 is competing against systems that are trained with large amounts of labeled data, it lags far behind . The way to think about these results is as follows: GPT-3 was not trained on these tasks explicitly; it was just trained as a language model to predict the next word. Nonetheless, even without “trying” , GPT-3 does a passable job on average at a broad range of NLP tasks. Because GPT-3 was not trained on any of these tasks, it hasn’t overfit, which means it has a good chance of doing well at many many other tasks (as seen by the passable performance on one-off tasks). Moreover, if you wanted to do well on any particular task (e.g., question answering), you should in principle be able to adapt GPT-3 using the large amounts of labeled data to exceed state-of-the-art. Adaptation . Recall that a language model \\\\(p\\\\) is a distribution over sequences of tokens \\\\(x_{1:L}\\\\) and thus can be used to score sequences: \\\\[p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}).\\\\] It can also be used to perform conditional generation of a completion given a prompt: \\\\[\\\\nl{the mouse ate} \\\\generate{} \\\\nl{the cheese}.\\\\] A task is a mapping from inputs to outputs. For example, for question answering, we might have: Input: What school did burne hogarth establish? Output: School of Visual Arts We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given: a natural language description of the task, and a set of training instances (input-output pairs). There are two primary ways to perform adaptation: Training (standard supervised learning): train a new model that maps inputs to outputs, either by creating a new model that uses the language model as features (probing), or starting with the language model and updating it based on the training instances (fine-tuning), or something in between (lightweight fine-tuning). Prompting (in-context learning): Construct a prompt (a string based on the description and training instances) or a set of prompts, feed those into a language model to obtain completions. Zero-shot learning: number of training examples is 0 One-shot learning: number of training examples is 1 Few-shot learning: number of training examples is few Which adaptation procedure should we go with? Training can be challenging due to overfitting (just imagine fine-tuning a 175 billion parameter model based on 5 examples). How to do this effectively will be the topic of the adaptation lecture. For now, we will be content with adaptation of GPT-3 using prompting . Note that the limitation of prompting is that we can only leverage a only small number of training instances (as many as can fit into a prompt). This is due to a limitation of Transformers, where the prompt and the completion must fit into 2048 tokens. The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following: Definition : What is the task and its motivation? Adaptation : How do we reduce the task to language modeling (via prompting)? Results : What are the quantitative numbers compared to task-specific state-of-the-art models? Size and number of examples matters . By default, the results will based on the full GPT-3 model (davinci), which has 175 billion parameters using in-context learning with as many training instances as you can stuff into the prompt. Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better. The tasks are grouped as follows: Language modeling Question answering Translation Arithmetic News article generation Novel tasks The goals of this lecture is to provide: an overview of tasks in NLP (independent of large language models), a sense of how well GPT-3 works, and a taste for the art of prompt engineering. Language modeling The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\\\(p\\\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\\\(x_{1:L}\\\\), for example: \\\\[\\\\nl{the mouse ate the cheese}\\\\] We can ask: what is the probability the language model assigns to it? \\\\[p(\\\\nl{the mouse ate the cheese})\\\\] Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: \\\\[p(x_{1:L}) = \\\\prod_{i=1}^L p(x_i \\\\mid x_{1:i-1}).\\\\] Perplexity . The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\\\(p(x_i \\\\mid x_{1:i-1})\\\\). We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that. Instead, we want the geometric average , which is exactly what perplexity does: \\\\[\\\\perplexity_p(x_{1:L}) = \\\\exp\\\\left(\\\\frac{1}{L} \\\\sum_{i=1}^L \\\\log \\\\frac{1}{p(x_i \\\\mid x_{1:i-1})}\\\\right).\\\\] Perplexity can be interpreted as the average “branching factor” per token. Recall that \\\\(\\\\log \\\\frac{1}{p(x_i \\\\mid x_{1:i-1})}\\\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\\\(2^3\\\\) possible strings. Tale of two errors . There are two types of errors a language model can make, and perplexity treats them asymmetrically: Recall error : The language model fails to place probability mass on some token. Perplexity has no mercy: \\\\[p(\\\\nl{ate} \\\\mid \\\\nl{the}, \\\\nl{mouse}) \\\\to 0 \\\\quad\\\\Rightarrow\\\\quad \\\\perplexity_p(\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}) \\\\to \\\\infty.\\\\] Precision error : The language model places extra probability mass on some bad sequences. Perplexity provides a slap on the wrist. Given a language model \\\\(p\\\\), suppose we mix in some garbage distribution \\\\(r\\\\) with probability \\\\(\\\\epsilon\\\\): \\\\[q(x_i \\\\mid x_{1:i-1}) = (1-\\\\epsilon) p(x_i \\\\mid x_{1:i-1}) + \\\\epsilon r(x_i \\\\mid x_{1:i-1}).\\\\] Then we can compute the perplexity of \\\\(x_{1:L}\\\\) under \\\\(q\\\\): \\\\[\\\\perplexity_q(x_{1:L}) \\\\le \\\\frac{1}{1 - \\\\epsilon} \\\\perplexity_p(x_{1:L}) \\\\approxeq (1 + \\\\epsilon) \\\\perplexity_p(x_{1:L}),\\\\] where the last approximate equality holds for small values of \\\\(\\\\epsilon\\\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token. Now let’s get on with evaluating perplexity on an actual dataset. Penn Tree Bank The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012) , a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out). Adaptation . Feed the entire text as a prompt into GPT-3 and evaluate the perplexity ( demo ): Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. Results . GPT-3 vastly outperforms the existing state-of-the-art: Model Perplexity GPT-3 20.5 BERT-Large-CAs1 31.3 See the leaderboard for the latest results. Train/test leakage . The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. LAMBADA ( Paperno et al. 2016 ) Task: predict the last word of a sentence. Motivation: Solving the task requires modeling long-range dependencies . Adaptation . LAMBADA is natively already a language modeling task, so we could just ask a language model to complete the final word of the sentence. Problem: language model doesn’t know it should be producing the final word of the sentence. Solution: frame it more explicitly as a input-output mapping and use in-context learning with additional examples ( demo ): Fill in blank: Alice was friends with Bob. Alice went to visit her friend ___. -> Bob She held the torch in front of her. She caught her breath. “Chris? There’s a step.” “What?” “A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step Results . GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2): Model Perplexity GPT-3 (few-shot) 1.92 SOTA 8.63 See the leaderboard for the latest results. HellaSwag ( Zellers et al. 2019 ) Motivation: evaluate a model’s ability to perform commonsense reasoning Task: choose the most appropriate completion for a sentence from a list of choices Adaptation . This is a multiple-choice task , so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one ( demo ): Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer} where ${answer} is one of: bake them, then frost and decorate. taste them as they place them on plates. put the frosting on the cake as they pan it. come out and begin decorating the cake as well. How do you score a candidate answer \\\\(y\\\\) given a question \\\\(x\\\\)? There’s no principled answer, but here are some heuristics : Unnormalized probability: \\\\(\\\\text{score}(x, y) = p(x, y)\\\\). The problem with the unnormalized probability is that it has a bias towards short answers ( demo ). Length-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(x, y)}{\\\\text{num-tokens}(y)}\\\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity. Frequency-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(y \\\\mid x)}{p(y \\\\mid x_0)}\\\\), where \\\\(x_0\\\\) is a neutral string like \\\\(\\\\nl{Answer:}\\\\). This lowers the score for answers that happen to just be common (e.g., \\\\nl{John}). Compare demo versus demo . Results . GPT-3 got close but did not exceed the state-of-the-art: Model Accuracy SOTA 85.6 GPT-3 79.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See the leaderboard for the latest results. Question answering Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided). Input: What school did burne hogarth establish? Output: School of Visual Arts TriviaQA ( Joshi et al. 2017 ) Task: given a trivia question, generate the answer The original dataset was collected from trivial enthusiasts and was presented as a challenge used for (open book) reading comprehension, but we use it for (closed-book) question answering. Adaptation . We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer ( demo ): Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Marcel Duchamp Results . Model Accuracy RAG 68.0 GPT-3 (zero-shot) 64.3 GPT-3 (few-shot) 71.2 We also see that both increasing the model size and the number of in-context training instances helps: WebQuestions ( Berant et al. 2013 ) Task: answer questions Dataset collected from Google search queries, initially created for question answering on knowledge bases Adaptation . We define a prompt the same as above ( demo ): Q: What school did burne hogarth establish? A: School of Visual Arts Results . Model Accuracy RAG 45.5 GPT-3 (zero-shot) 14.4 GPT-3 (few-shot) 41.5 NaturalQuestions Task: answer questions Dataset collected from Google search queries (with long-form answers) Adaptation . We define a prompt the same as above ( demo ): Q: Who played tess on touched by an angel? A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese. Results . Model Accuracy RAG 44.5 GPT-3 (zero-shot) 14.6 GPT-3 (few-shot) 29.9 Translation Task: translate a sentence in a source language (e.g., German) to sentence in a target language (e.g., English) Machine translation has been a long standing NLP task since the 1960s, and statistical machine translation took off within NLP (with its own distinct subcommunity) in the 2000s, followed by neural machine translation in the mid-2010s. It has always been a data-rich field due to the existence of human translators. The standard evaluation dataset is the WMT’14 and WMT’16 datasets. Since there are multiple possible translations, the (automatic) evaluation metric is BLEU (which captures a notion of n-gram overlap). Adaptation . For the few-shot setting, we construct a prompt containing input-output training instances along with the input ( demo ): Mein Haus liegt auf dem Hügel. = My house is on the hill. Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. = In no case may they be used for commercial purposes. Results . Here are the results from German to English: Model Accuracy SOTA (supervised) 40.2 GPT-3 (zero-shot) 27.2 GPT-3 (few-shot) 40.6 Even without supervised training data, GPT-3 matches the state-of-the-art of a fully-supervised system! This presents a lower bound on how well one can do in machine translation; you would definitely want to leverage the large amount of parallel corpora (aligned input-output pairs). Results from French and Romanian are similar. Results from English to a foreign language is much worse, which is expected since GPT-3 is primarily an English language model. Arithmetic GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model. Task: do arithmetic (2-5 digit addition, subtraction, multiplication) There’s no practical reason you would want to solve this task; it’s just a diagnostic task to satisfy our scientific curiosity. Adaptation . Pose the problem as question answering ( demo ): Q: What is 556 plus 497? A: 1053 Results . It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well. News article generation Task: given title and subtitle, generate a news article Dataset: title/subtitles taken from newser.com Evaluation: humans rated articles based on how likely the article was likely to be written by a machine Adaptation . Note: in-context learning was needed to give the model an idea of what a prompt looks like. Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination... Results . Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance). For the article above, humans guessed “machine” correctly only 12% of the time. Novel tasks Using new words Task: given a new made-up word and a definition, generate a sentence that uses the word. Adaptation . Just describe the task in the prompt ( demo ): To “screeg” something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeged the tree with our swords. Correcting English grammar Task: given an ungrammatical sentence, generate its grammatical version. Adaptation . The prompt consists of input-output pairs ( demo ): Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I would be happy to work with you on another project. Other tasks Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks . SWORDS : lexical substitution, where the goal is to predict synonyms in the context of a sentence. Massive Multitask Language Understanding : 57 multiple-choice problems spanning mathematics, US history, computer science, law, etc. TruthfulQA : question answering dataset that humans would answer falsely due to misconceptions. The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning! Demos . Examples from the OpenAI website Examples from gpt3demo.com The demos are creative and interesting, but it’s hard to tell how reliably they work. Summary GPT-3 was evaluated on a wide range of standard NLP benchmarks and on quirky one-off tasks. GPT-3 can perform extremely well or be very medicore. Both increasing the size of the model and the number of examples helps performance. There are a few heuristic ways of adapting the language model to the task of interest. Why does this work? No one knows. Further reading Language Models are Few-Shot Learners . Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei . NeurIPS 2020. Blog post explaining perplexity',\n",
       "  'links': ['https://stanford-cs324.github.io/winter2022/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/',\n",
       "   'https://stanford-cs324.github.io/winter2022/calendar/',\n",
       "   '#',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/introduction/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/capabilities/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-1/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/harms-2/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/data/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/security/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/legality/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/modeling/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/training/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/parallelism/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/adaptation/',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/environment/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-reviews/',\n",
       "   'https://stanford-cs324.github.io/winter2022/paper-discussions/',\n",
       "   'https://stanford-cs324.github.io/winter2022/projects/',\n",
       "   'https://github.com/pmarsceill/just-the-docs',\n",
       "   'https://stanford-cs324.github.io/winter2022/lectures/',\n",
       "   'https://arxiv.org/pdf/2005.14165.pdf',\n",
       "   '#language-modeling',\n",
       "   '#question-answering',\n",
       "   '#translation',\n",
       "   '#arithmetic',\n",
       "   '#news-article-generation',\n",
       "   '#novel-tasks',\n",
       "   '#language-modeling',\n",
       "   '#penn-tree-bank',\n",
       "   'https://catalog.ldc.upenn.edu/LDC99T42',\n",
       "   'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1325968',\n",
       "   'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6424228',\n",
       "   'https://nlp.stanford.edu/~johnhew/',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Pierre%20Vinken%2C%2061%20years%20old%2C%20will%20join%20the%20board%20as%20a%20nonexecutive%20director%20Nov.%2029.%20%20Mr.%20Vinken%20is%20chairman%20of%20Elsevier%20N.V.%2C%20the%20Dutch%20publishing%20group.&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200%0Atop_k_per_token%3A%205%0Amodel%3A%20%24%7Bmodel%7D&environments=model%3A%20%5Bopenai%2Fdavinci%2C%20openai%2Fcurie%2C%20ai21%2Fj1-jumbo%5D',\n",
       "   'https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word',\n",
       "   'https://paperswithcode.com/dataset/wikitext-103',\n",
       "   '#lambada-paperno-et-al-2016',\n",
       "   'https://arxiv.org/pdf/1606.06031.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Fill%20in%20blank%3A%0A%0AAlice%20was%20friends%20with%20Bob.%20Alice%20went%20to%20visit%20her%20friend%20___.%20-%3E%20Bob%0A%0AShe%20held%20the%20torch%20in%20front%20of%20her.%0AShe%20caught%20her%20breath.%0A%22Chris%3F%20%20There%E2%80%99s%20a%20step.%22%0A%22What%3F%22%0A%22A%20step.%20Cut%20in%20the%20rock.%20About%20fifty%20feet%20ahead.%22%20She%20moved%20faster.%20They%20both%20moved%20faster.%20%22In%20fact%2C%22%20she%20said%2C%20raising%20the%20torch%20higher%2C%20%22there%E2%80%99s%20more%20than%20a%20___.%20-%3E&settings=temperature%3A%200%0Amax_tokens%3A%201%0Atop_k_per_token%3A%2010%0Amodel%3A%20%24%7Bmodel%7D&environments=model%3A%20%5Bopenai%2Fdavinci%2C%20openai%2Fcurie%2C%20ai21%2Fj1-jumbo%5D',\n",
       "   'https://paperswithcode.com/sota/language-modelling-on-lambada',\n",
       "   '#hellaswag-zellers-et-al-2019',\n",
       "   'https://arxiv.org/pdf/1905.07830.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Making%20a%20cake%3A%20Several%20cake%20pops%20are%20shown%20on%20a%20display.%20%20A%20woman%20and%20girl%20are%20shown%20making%20the%20cake%20pops%20in%20a%20kitchen.%20%0A%20They%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%205%0Aecho_prompt%3A%20true&environments=answer%3A%20%5B%22bake%20them%2C%20then%20frost%20and%20decorate.%22%2C%20%22taste%20them%20as%20they%20place%20them%20on%20plates.%22%2C%20%22put%20the%20frosting%20on%20the%20cake%20as%20they%20pan%20it.%22%2C%20%22come%20out%20and%20begin%20decorating%20the%20cake%20as%20well.%22%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=Question%3A%20Why%20is%20the%20sky%20blue%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BIt%27s%20due%20to%20a%20phenomenon%20called%20Raleigh%20scattering%2C%20Because%20Mars%20is%20red%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=John%20pushed%20Khaleesi.%0AQuestion%3A%20Who%20was%20upset%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BJohn%2C%20Khaleesi%5D',\n",
       "   'http://localhost:1959/static/index.html?prompt=John%20pushed%20Bob.%0AQuestion%3A%20Who%20was%20upset%3F%0AAnswer%3A%20%24%7Banswer%7D&settings=temperature%3A%200%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010%0Aecho_prompt%3A%20true&environments=answer%3A%20%5BJohn%2C%20Bob%5D',\n",
       "   'https://paperswithcode.com/sota/sentence-completion-on-hellaswag',\n",
       "   '#question-answering',\n",
       "   '#triviaqa-joshi-et-al-2017',\n",
       "   'https://arxiv.org/pdf/1705.03551.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20%E2%80%98Nude%20Descending%20A%20Staircase%E2%80%99%20is%20perhaps%20the%20most%20famous%20painting%20by%20which%0A20th%20century%20artist%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#webquestions-berant-et-al-2013',\n",
       "   'https://aclanthology.org/D13-1160.pdf',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20What%20school%20did%20burne%20hogarth%20establish%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#naturalquestions',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20Who%20played%20tess%20on%20touched%20by%20an%20angel%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#translation',\n",
       "   'https://paperswithcode.com/dataset/wmt-2014',\n",
       "   'https://paperswithcode.com/dataset/wmt-2016',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Mein%20Haus%20liegt%20auf%20dem%20H%C3%BCgel.%20%3D%20My%20house%20is%20on%20the%20hill.%0AKeinesfalls%20d%C3%BCrfen%20diese%20f%C3%BCr%20den%20kommerziellen%20Gebrauch%20verwendet%20werden.%20%3D&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#arithmetic',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Q%3A%20What%20is%20556%20plus%20497%3F%0AA%3A&settings=temperature%3A%200%0Astop_sequences%3A%20%5B%5Cn%5D&environments=',\n",
       "   '#news-article-generation',\n",
       "   'newser.com',\n",
       "   '#novel-tasks',\n",
       "   '#using-new-words',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=To%20%22screeg%22%20something%20is%20to%20swing%20a%20sword%20at%20it.%20%20An%20example%20of%20a%20sentence%20that%20uses%20the%20word%20screeg%20is%3A%0AWe&settings=stop_sequences%3A%20%5B%5Cn%5D%0Anum_completions%3A%2010%0Atemperature%3A%200.5&environments=',\n",
       "   '#correcting-english-grammar',\n",
       "   'http://crfm-models.stanford.edu/static/index.html?prompt=Poor%20English%20input%3A%20I%20eated%20the%20purple%20berries.%0AGood%20English%20output%3A%20I%20ate%20the%20purple%20berries.%0APoor%20English%20input%3A%20Thank%20you%20for%20picking%20me%20as%20your%20designer.%20I%E2%80%99d%20appreciate%20it.%0AGood%20English%20output%3A%20Thank%20you%20for%20choosing%20me%20as%20your%20designer.%20I%20appreciate%20it.%0APoor%20English%20input%3A%20The%20mentioned%20changes%20have%20done.%20or%20I%20did%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20did%20the%20modifications.%0AGood%20English%20output%3A%20The%20requested%20changes%20have%20been%20made.%20or%20I%20made%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20made%20the%20modifications.%0APoor%20English%20input%3A%20I%E2%80%99d%20be%20more%20than%20happy%20to%20work%20with%20you%20in%20another%20project.%0AGood%20English%20output%3A&settings=stop_sequences%3A%20%5B%5Cn%5D%0Atemperature%3A%200%0Atop_k_per_token%3A%205&environments=',\n",
       "   '#other-tasks',\n",
       "   'https://arxiv.org/pdf/2106.04102.pdf',\n",
       "   'https://arxiv.org/pdf/2009.03300.pdf',\n",
       "   'https://arxiv.org/pdf/2109.07958.pdf',\n",
       "   'https://beta.openai.com/examples/',\n",
       "   'https://gpt3demo.com/',\n",
       "   '#summary',\n",
       "   '#further-reading',\n",
       "   'https://arxiv.org/pdf/2005.14165.pdf',\n",
       "   'https://towardsdatascience.com/perplexity-in-language-models-87a196019a94'],\n",
       "  'tables': [[['ModelPerplexityGPT-320.5BERT-Large-CAs131.3',\n",
       "     'PerplexityGPT-320.5BERT-Large-CAs131.3',\n",
       "     'GPT-320.5BERT-Large-CAs131.3',\n",
       "     '20.5BERT-Large-CAs131.3',\n",
       "     'BERT-Large-CAs131.3',\n",
       "     '31.3'],\n",
       "    ['GPT-320.5BERT-Large-CAs131.3',\n",
       "     '20.5BERT-Large-CAs131.3',\n",
       "     'BERT-Large-CAs131.3',\n",
       "     '31.3'],\n",
       "    ['BERT-Large-CAs131.3', '31.3']],\n",
       "   [['ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63',\n",
       "     'PerplexityGPT-3 (few-shot)1.92SOTA8.63',\n",
       "     'GPT-3 (few-shot)1.92SOTA8.63',\n",
       "     '1.92SOTA8.63',\n",
       "     'SOTA8.63',\n",
       "     '8.63'],\n",
       "    ['GPT-3 (few-shot)1.92SOTA8.63', '1.92SOTA8.63', 'SOTA8.63', '8.63'],\n",
       "    ['SOTA8.63', '8.63']],\n",
       "   [['ModelAccuracySOTA85.6GPT-379.3',\n",
       "     'AccuracySOTA85.6GPT-379.3',\n",
       "     'SOTA85.6GPT-379.3',\n",
       "     '85.6GPT-379.3',\n",
       "     'GPT-379.3',\n",
       "     '79.3'],\n",
       "    ['SOTA85.6GPT-379.3', '85.6GPT-379.3', 'GPT-379.3', '79.3'],\n",
       "    ['GPT-379.3', '79.3']],\n",
       "   [['ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'AccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'RAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     '68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     '64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (few-shot)71.2',\n",
       "     '71.2'],\n",
       "    ['RAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     '68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     '64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (few-shot)71.2',\n",
       "     '71.2'],\n",
       "    ['GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2',\n",
       "     '64.3GPT-3 (few-shot)71.2',\n",
       "     'GPT-3 (few-shot)71.2',\n",
       "     '71.2'],\n",
       "    ['GPT-3 (few-shot)71.2', '71.2']],\n",
       "   [['ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'AccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'RAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     '45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     '14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (few-shot)41.5',\n",
       "     '41.5'],\n",
       "    ['RAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     '45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     '14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (few-shot)41.5',\n",
       "     '41.5'],\n",
       "    ['GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5',\n",
       "     '14.4GPT-3 (few-shot)41.5',\n",
       "     'GPT-3 (few-shot)41.5',\n",
       "     '41.5'],\n",
       "    ['GPT-3 (few-shot)41.5', '41.5']],\n",
       "   [['ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'AccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'RAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     '44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     '14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (few-shot)29.9',\n",
       "     '29.9'],\n",
       "    ['RAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     '44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     '14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (few-shot)29.9',\n",
       "     '29.9'],\n",
       "    ['GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9',\n",
       "     '14.6GPT-3 (few-shot)29.9',\n",
       "     'GPT-3 (few-shot)29.9',\n",
       "     '29.9'],\n",
       "    ['GPT-3 (few-shot)29.9', '29.9']],\n",
       "   [['ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'AccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'SOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     '40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     '27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (few-shot)40.6',\n",
       "     '40.6'],\n",
       "    ['SOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     '40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     '27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (few-shot)40.6',\n",
       "     '40.6'],\n",
       "    ['GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6',\n",
       "     '27.2GPT-3 (few-shot)40.6',\n",
       "     'GPT-3 (few-shot)40.6',\n",
       "     '40.6'],\n",
       "    ['GPT-3 (few-shot)40.6', '40.6']]],\n",
       "  'images': ['../images/gpt3-trivia-qa-performance.png',\n",
       "   '../images/gpt3-arithmetic-performance.png']}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
