# AI-QueryBot

**AI-QueryBot** is a Question-Answering chatbot that leverages Retrieval-Augmented Generation (RAG) with large language models (LLMs). This project enables users to ask questions and receive answers generated by an LLM while retrieving relevant documents to enhance the response accuracy. It is inspired by the RAG Q-A chatbot project from Kaggle and has been extended with additional functionality, including a Gradio UI for an interactive chat experience.

## Table of Contents

1. [Overview](#overview)
2. [Features](#features)
3. [Technologies](#technologies)
4. [Installation](#installation)
5. [How It Works](#how-it-works)
6. [RAG Overview](#rag-overview)
7. [Dataset](#dataset)
9. [Contributing](#contributing)


## Overview

This project is a Query BOT that uses Retrieval-Augmented Generation (RAG) techniques to handle natural language queries with accuracy and context-awareness. It’s built around Llama 2, accessed through the Hugging Face pipeline. To keep it lightweight, the model is quantized to 4-bit, which means it uses significantly less memory without sacrificing much in performance. This way, the BOT can generate flexible and coherent responses even on resource-limited hardware, and settings like temperature and top-p sampling are adjustable to refine how the BOT responds.

To make large documents easily searchable, we split them into smaller, manageable chunks using LangChain’s RecursiveCharacterTextSplitter. Each chunk is then turned into a vector with sentence-transformers and stored in a FAISS vector store, a high-speed vector database that makes it quick to search for similar text. This vector database serves as the BOT’s knowledge base, allowing it to retrieve relevant information in response to each query, which keeps responses precise and relevant.

At the heart of the BOT’s functionality is LangChain’s RetrievalQA chain, which ties together the retrieval and response generation steps. The FAISS vector store retrieves relevant chunks, and Maximum Marginal Relevance (MMR) is applied to ensure the returned chunks are diverse, avoiding redundant information. This setup ensures that the BOT’s answers are not only accurate but also well-rounded, drawing from a broad context to address the query comprehensively.

Finally, the BOT’s output is carefully formatted to improve readability and usability. Each answer is structured for clarity, with text wrapping to maintain readability, and it includes source information, listing document names and page numbers so users can trace where the information came from. The BOT also shows how long it took to generate the answer, making the response more transparent. This combination of retrieval, generation, and formatting makes the Query BOT an efficient and dependable tool for answering complex questions in knowledge-heavy domains.

## Features

- **Retrieval-Augmented Generation (RAG):** Combines retrieval and generation models for better question-answering.
- **LLM Integration:** Leverages large language models for natural language understanding and response generation.
- **Document Retrieval:** Retrieves relevant information from a corpus of documents to assist in answering questions.
- **Interactive UI:** Includes a Gradio-based chat UI for easy interaction with the bot.
- **Customizable Corpus:** Users can upload their own documents to create a domain-specific Q&A bot.

## Technologies

This project uses the following key technologies and frameworks:

- **Python 3.8+**
- **Transformers (Hugging Face)**: For working with pre-trained LLMs.
- **Gradio**: For building the web-based user interface.
- **LangChain**: For chaining LLMs and managing complex workflows.
- **FAISS**: For efficient document retrieval.
- **Pandas**: For data processing.
- **PyTorch**: As the backend for training and fine-tuning models.

## Installation

To set up **AI-QueryBot** locally, follow these steps:

### 1. Clone the Repository
```bash
git clone https://github.com/S1ayer2602/AI-QueryBot.git
cd AI-QueryBot
```
### 2. Create a Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```
### 3. Install Dependencies
```bash
pip install -r requirements.txt

```

## How It Works
- **Question Processing**: The user submits a question, which is processed by the language model.
- **Document Retrieval**: A search is performed over the document corpus using FAISS (or another retrieval mechanism), retrieving the most relevant documents related to the question.
- **Answer Generation**: The model generates a response by combining the user query with the retrieved documents. This is where RAG (Retrieval-Augmented Generation) comes into play.
- **Interactive UI**: The answer is displayed in the chat UI, along with any relevant supporting documents.
## RAG Overview
- **Retrieval Component**: FAISS or another vector store is used to fetch relevant documents from the document index based on the query.
- **Generation Component**: A pre-trained LLM is used to generate a response, conditioned on the query and the retrieved documents.

## Dataset
The current version uses publicly available datasets (e.g., from Wikipedia or similar sources), but it is fully customizable. You can upload your own documents for domain-specific QA.

## Contributing
Contributions are welcome! If you have ideas for new features or find bugs, feel free to open an issue or submit a pull request.
